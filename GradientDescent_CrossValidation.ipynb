{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente por descenso\n",
    "#### Equipo No. 6: Elena Villalobos Nolasco, Carolina Acosta Tovany y Aide Jazmín González Cruz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.- Se importan los paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.- Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, header):\n",
    "    marks_df = pd.read_csv(path, header=header)\n",
    "    return marks_df\n",
    "\n",
    "data = load_data(\"iris_num.txt\",None)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.- Definimos nuestra función costo/objetivo (LOSS Function)\n",
    "\n",
    "$J(\\theta,\\theta_0) = \\frac{1}{n} \\sum_{i=1}^{n} L_h (y^{(i)}(\\theta^{T}x^{i} + \\theta_0)) + \\frac{\\lambda}{2} || \\theta || ^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costo(teta,teta0,data,n,lamda):#Función Objetivo J(Ø,Ø0)\n",
    "    suma = np.array([0.0])\n",
    "    yi = 0.0\n",
    "    modulo = 0.0\n",
    "    modulo = (teta[0]*teta[0]) + (teta[1]*teta[1]) + (teta[2]*teta[2]) + (teta[3]*teta[3])\n",
    "    \n",
    "    for i in data:\n",
    "        if(i[4] == 0.0):\n",
    "            yi = -1.0\n",
    "        else:\n",
    "            yi = 1.0\n",
    "                \n",
    "        x = i[:len(i)-1]       \n",
    "              \n",
    "        ##(yi(ØTxi + Ø0))\n",
    "        v = yi * ((np.dot(np.array(teta).transpose(),np.array(x))) + teta0)\n",
    "        \n",
    "        ##SUMA Lh(yi(ØTxi + Ø0))\n",
    "        if v < 1.0:\n",
    "            Lh = (1.0 - v)\n",
    "        else:\n",
    "            Lh = 0.0\n",
    "            \n",
    "        suma =  suma + (Lh + ((lamda/2)*modulo))\n",
    "        \n",
    "    return ((1/n)*suma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.- Definimos nuestra función descenso por gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descenso_x_gradiente(data, eta, Oini, O0ini, lamda, epsilon):\n",
    "        # Se crea arreglo para guardar los pesos que se van calculando               \n",
    "        O = []\n",
    "        O0 = []\n",
    "        O.append(Oini)\n",
    "        O0.append(O0ini)\n",
    "        \n",
    "        Ot = [0.0, 0.0, 0.0, 0.0]\n",
    "        Ot0 = [0.0]\n",
    "        \n",
    "        t = 0\n",
    "        \n",
    "        n = len(data)\n",
    "        \n",
    "        while True:  \n",
    "            t = t + 1\n",
    "            sumaO = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "            sumaO0 = np.array([0.0])\n",
    "            \n",
    "            for i in data:\n",
    "                \n",
    "                if(i[4] == 0.0):\n",
    "                    yi = -1.0\n",
    "                else:\n",
    "                    yi = 1.0              \n",
    "                                    \n",
    "                x = i[:len(i)-1]\n",
    "                \n",
    "                ##Valida esta parte y(i)(Ø(t-1)Tx(i)+Ø0(t-1)) \n",
    "                hip = yi * ((np.dot(np.array(O[t-1]).transpose(),np.array(x))) + O0[t-1])\n",
    "                \n",
    "                val = 0.0\n",
    "                if(hip < 1.0):\n",
    "                    val = -1.0\n",
    "                else:\n",
    "                    val = 0.0\n",
    "                    \n",
    "                hip_O = (val * (yi * np.array(x))) + (lamda * O[t-1])\n",
    "                hip_O0 = val * yi\n",
    "                \n",
    "                sumaO = sumaO + hip_O\n",
    "                sumaO0 = sumaO0 + hip_O0                        \n",
    "            \n",
    "            Ot = O[t-1] - eta * ((1/n)*sumaO)\n",
    "            Ot0 = O0[t-1] - eta * (((1/n)*sumaO0))\n",
    "                                 \n",
    "            O = np.insert(O, t, Ot, axis=0)\n",
    "            O0 = np.insert(O0, t, Ot0, axis=0)\n",
    "            \n",
    "            cost = abs(costo(O[t],O0[t],data,n,lamda) - costo(O[t-1],O0[t-1],data,n,lamda))     \n",
    "            \n",
    "                                   \n",
    "            #Se corre el while hasta alcanzar el epsilon o las iteraciones son mayor a 1000 (para no tender a loop infinito)    \n",
    "            if((cost < epsilon) or (t > 1000)):\n",
    "                break \n",
    "        \n",
    "        #print('t: ' + str(t))      \n",
    "        return [O[t], O0[t]]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.- Definimos nuestra función de validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1993)\n",
    "\n",
    "def CrossValidation(data,K):\n",
    "    lamda_list = [100,10,1,0.1]\n",
    "    eta = 0.01 # LearningRate\n",
    "    epsilon = 0.05\n",
    "    df_lamda = pd.DataFrame(0.0, index=range(1),columns=(\"100\",\"10\",\"1\",\"0.1\"))\n",
    "    \n",
    "    # Dividimos los datos en chunk de igual tamaño\n",
    "    chunks = np.array_split(data, K)\n",
    "    for lamda_ in lamda_list:\n",
    "        error = 0.0\n",
    "        for i in range(1,int(K)):\n",
    "            \n",
    "            # Inicializamos las tetas\n",
    "            Oini = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "            O0ini = np.array([0.0])\n",
    "            \n",
    "            # El chunk i será nuestro bloque de datos de prueba\n",
    "            test_data = chunks[i]\n",
    "            \n",
    "            # Entrenamos con todos los otros datos menos con el chunk[i]\n",
    "            train_data = chunks.copy()\n",
    "            train_data.pop(i)\n",
    "            train_data = np.concatenate(train_data)\n",
    "            tetas = descenso_x_gradiente(train_data, eta, Oini, O0ini, lamda_, epsilon)\n",
    "            \n",
    "            #  Obtenemos el error con la función objetivo\n",
    "            error += costo(tetas[0],tetas[1],test_data,len(test_data),lamda_)\n",
    "        error /= K\n",
    "        \n",
    "        df_lamda[str(lamda_)][0] = error\n",
    "        \n",
    "    print(df_lamda)\n",
    "    return (df_lamda.idxmin(axis=1)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.- Hacemos la validación cruzada sobre nuestra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        100        10         1       0.1\n",
      "0  0.720983  0.427988  0.277125  0.268085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lamda = CrossValidation(data,5.0)\n",
    "float(df_lamda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión:\n",
    "\n",
    "De acuerdo al CrossValidation , debemos utilizar lambda 0.1 para obtener mejor resultados, porque tendrá menor error segun la funcion de costo. \n",
    "\n",
    "Estamos cosiderando un eta de 0.01 y un epsilon de 0.05\n",
    "\n",
    "#### 8.- Volvemos a entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teta :  [0.14013694 0.1001023  0.03228874 0.00318462]  \n",
      "Teta_0: [0.029]\n"
     ]
    }
   ],
   "source": [
    "#Se revuelven los datos\n",
    "random.shuffle(data)\n",
    "\n",
    "#Datos de entrenamiento\n",
    "datos_entrenamiento = data[0:120]\n",
    "\n",
    "#Datos de verificación\n",
    "data_validacion = data[121:150]\n",
    "\n",
    "\n",
    "eta = 0.01\n",
    "epsilon = 0.05\n",
    "Oini = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "O0ini = np.array([0.0])\n",
    "\n",
    "tetas = descenso_x_gradiente(datos_entrenamiento, eta, Oini, O0ini, float(df_lamda), epsilon)\n",
    "print(\"Teta : \", tetas[0], \" \\nTeta_0:\", tetas[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
